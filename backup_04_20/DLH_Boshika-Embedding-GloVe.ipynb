{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "FILE_NAME_SUFFIX = \".multi_admission\" #\"_1000\"\n",
    "DATASET_PATH = \"Datasets/\"\n",
    "\n",
    "def read_samples(input_file, file_name_suffix):\n",
    "    file_name = DATASET_PATH + input_file + file_name_suffix\n",
    "    return pd.read_csv(file_name, error_bad_lines=False, keep_default_na=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "patients = read_samples(\"PATIENTS.csv\", FILE_NAME_SUFFIX)\n",
    "admissions = read_samples(\"ADMISSIONS.csv\", FILE_NAME_SUFFIX)\n",
    "admissions[\"admit_date\"] = admissions[\"ADMITTIME\"].apply(lambda s: (datetime.datetime.strptime(s, '%Y-%m-%d %H:%M:%S') - datetime.datetime(1970,1,1)).days)\n",
    "admissions[\"discharge_date\"] = admissions[\"DISCHTIME\"].apply(lambda s: (datetime.datetime.strptime(s, '%Y-%m-%d %H:%M:%S') - datetime.datetime(1970,1,1)).days)\n",
    "admissions['LOS'] = admissions[\"discharge_date\"] - admissions[\"admit_date\"]\n",
    "\n",
    "diagnoses = read_samples(\"DIAGNOSES_ICD.csv\", FILE_NAME_SUFFIX)\n",
    "procedures = read_samples(\"PROCEDURES_ICD.csv\", FILE_NAME_SUFFIX)\n",
    "icu_stays = read_samples(\"ICUSTAYS.csv\", FILE_NAME_SUFFIX)\n",
    "notes = read_samples(\"NOTEEVENTS.csv\", FILE_NAME_SUFFIX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit the patients to the ones with more than 1 admission:\n",
      " count    7537.000000\n",
      "mean        2.652647\n",
      "std         1.621112\n",
      "min         2.000000\n",
      "25%         2.000000\n",
      "50%         2.000000\n",
      "75%         3.000000\n",
      "max        42.000000\n",
      "Name: num_admissions, dtype: float64\n",
      "Prevalence of patients with more than 1 admission:  1.0\n",
      "Prevalence of patients with more than 2 admissions:  0.31537747114236436\n",
      "Prevalence of patients with more than 3 admissions:  0.13732254212551412\n",
      "Prevalence of patients with more than 4 admissions:  0.06992171951704923\n",
      "Prevalence of patients with more than 5 admissions:  0.03728273849011543\n"
     ]
    }
   ],
   "source": [
    "patients = patients.set_index(\"SUBJECT_ID\", drop=False)\n",
    "patients[\"num_admissions\"] = admissions.groupby(\"SUBJECT_ID\").size().to_frame(\"num_admissions\")\n",
    "patients = patients[patients.num_admissions > 1]\n",
    "print(\"Limit the patients to the ones with more than 1 admission:\\n\", patients.num_admissions.describe())\n",
    "print(\"Prevalence of patients with more than 1 admission: \", len(patients[patients[\"num_admissions\"] > 1]) / len(patients))\n",
    "print(\"Prevalence of patients with more than 2 admissions: \", len(patients[patients[\"num_admissions\"] > 2]) / len(patients))\n",
    "print(\"Prevalence of patients with more than 3 admissions: \", len(patients[patients[\"num_admissions\"] > 3]) / len(patients))\n",
    "print(\"Prevalence of patients with more than 4 admissions: \", len(patients[patients[\"num_admissions\"] > 4]) / len(patients))\n",
    "print(\"Prevalence of patients with more than 5 admissions: \", len(patients[patients[\"num_admissions\"] > 5]) / len(patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = admissions[admissions.SUBJECT_ID.isin(patients.SUBJECT_ID)]\n",
    "procedures = procedures[procedures.SUBJECT_ID.isin(patients.SUBJECT_ID)]\n",
    "diagnoses = diagnoses[diagnoses.SUBJECT_ID.isin(patients.SUBJECT_ID)]\n",
    "icu_stays = icu_stays[icu_stays.SUBJECT_ID.isin(patients.SUBJECT_ID)]\n",
    "notes = notes[notes.SUBJECT_ID.isin(patients.SUBJECT_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record length before the final admission\n",
      " count    7533.000000\n",
      "mean      242.149608\n",
      "std       579.288149\n",
      "min         0.000000\n",
      "25%         6.000000\n",
      "50%        13.000000\n",
      "75%        84.000000\n",
      "max      4145.000000\n",
      "Name: record_length, dtype: float64\n",
      "Interval before the final admission\n",
      " count    7533.000000\n",
      "mean      445.034382\n",
      "std       678.907514\n",
      "min       -19.000000\n",
      "25%        25.000000\n",
      "50%       131.000000\n",
      "75%       570.000000\n",
      "max      4108.000000\n",
      "Name: final_admission_interval, dtype: float64\n",
      "Prevalence of readmission in 30 days =  0.27517579938967757\n"
     ]
    }
   ],
   "source": [
    "last_admission = admissions[admissions.groupby(['SUBJECT_ID'])['admit_date'].transform(max) == admissions['admit_date']]\n",
    "previous_admissions = admissions[admissions.groupby(['SUBJECT_ID'])['admit_date'].transform(max) != admissions['admit_date']]\n",
    "patients[\"record_start_date\"] = previous_admissions.groupby(\"SUBJECT_ID\").admit_date.agg(['min'])\n",
    "patients[\"record_end_date\"] = previous_admissions.groupby(\"SUBJECT_ID\").discharge_date.agg(['max'])\n",
    "patients[\"record_length\"] = patients.record_end_date - patients.record_start_date\n",
    "patients[\"final_admission_date\"] = admissions.groupby(\"SUBJECT_ID\").admit_date.agg(['max'])\n",
    "patients[\"final_admission_interval\"] = patients.final_admission_date - patients.record_end_date\n",
    "\n",
    "print(\"Record length before the final admission\\n\", patients.record_length.describe())\n",
    "\n",
    "print(\"Interval before the final admission\\n\", patients.final_admission_interval.describe())\n",
    "\n",
    "print(\"Prevalence of readmission in 30 days = \", len(patients[patients.final_admission_interval < 30]) / len(patients))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prevalence of ICU admission in 30 days =  0.006103224094467295\n"
     ]
    }
   ],
   "source": [
    "icu_stays[\"admit_date\"] = icu_stays[\"INTIME\"].apply(lambda s: (datetime.datetime.strptime(s, '%Y-%m-%d %H:%M:%S') - datetime.datetime(1970,1,1)).days)\n",
    "final_icu_admission = icu_stays[icu_stays.groupby(['SUBJECT_ID'])['admit_date'].transform(max) != icu_stays['admit_date']]\n",
    "patients[\"final_icu_admission_date\"] = final_icu_admission.groupby(\"SUBJECT_ID\").admit_date.agg(['min'])\n",
    "patients[\"final_icu_admission_interval\"] = patients.final_icu_admission_date - patients.record_end_date\n",
    "print(\"Prevalence of ICU admission in 30 days = \", len(patients[(patients[\"final_icu_admission_interval\"] >= 0) & (patients[\"final_icu_admission_interval\"] < 30)]) / len(patients))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       procedure_freq_by_icd9_code\n",
      "count                  1513.000000\n",
      "mean                     54.035030\n",
      "std                     275.114179\n",
      "min                       1.000000\n",
      "25%                       1.000000\n",
      "50%                       4.000000\n",
      "75%                      19.000000\n",
      "max                    6505.000000\n",
      "       procedure_freq_by_patient\n",
      "count                7364.000000\n",
      "mean                   11.101983\n",
      "std                     8.891169\n",
      "min                     1.000000\n",
      "25%                     5.000000\n",
      "50%                     9.000000\n",
      "75%                    15.000000\n",
      "max                    98.000000\n",
      "       procedure_freq_by_admission\n",
      "count                 17393.000000\n",
      "mean                      4.700454\n",
      "std                       3.961372\n",
      "min                       1.000000\n",
      "25%                       2.000000\n",
      "50%                       3.000000\n",
      "75%                       6.000000\n",
      "max                      40.000000\n",
      "           freq\n",
      "ICD9_CODE      \n",
      "3893       6505\n",
      "9604       3440\n",
      "966        3203\n",
      "9904       2940\n",
      "9671       2796\n",
      "9672       2225\n",
      "3995       2030\n",
      "3891       1620\n",
      "8856       1497\n",
      "9915       1364\n",
      "3961       1364\n",
      "4513       1339\n",
      "3324       1316\n",
      "8872       1218\n",
      "5491       1078\n",
      "3895        982\n",
      "3722        920\n",
      "9907        828\n",
      "3491        824\n",
      "3723        792\n"
     ]
    }
   ],
   "source": [
    "print(procedures.groupby(\"ICD9_CODE\").size().to_frame(\"procedure_freq_by_icd9_code\").describe())\n",
    "print(procedures.groupby(\"SUBJECT_ID\").size().to_frame(\"procedure_freq_by_patient\").describe())\n",
    "print(procedures.groupby(\"HADM_ID\").size().to_frame(\"procedure_freq_by_admission\").describe())\n",
    "\n",
    "print(procedures.groupby(\"ICD9_CODE\").size().to_frame(\"freq\").sort_values(\"freq\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       diagnosis_freq_by_icd9_code\n",
      "count                  4894.000000\n",
      "mean                     53.192889\n",
      "std                     258.042880\n",
      "min                       1.000000\n",
      "25%                       1.000000\n",
      "50%                       4.000000\n",
      "75%                      20.000000\n",
      "max                    7183.000000\n",
      "       diagnosis_freq_by_patient\n",
      "count                7537.000000\n",
      "mean                   34.539737\n",
      "std                    28.731059\n",
      "min                     2.000000\n",
      "25%                    18.000000\n",
      "50%                    28.000000\n",
      "75%                    41.000000\n",
      "max                   540.000000\n",
      "       diagnosis_freq_by_admission\n",
      "count                 19993.000000\n",
      "mean                     13.020857\n",
      "std                       6.860812\n",
      "min                       1.000000\n",
      "25%                       9.000000\n",
      "50%                      11.000000\n",
      "75%                      17.000000\n",
      "max                      39.000000\n",
      "           freq\n",
      "ICD9_CODE      \n",
      "4019       7183\n",
      "4280       6588\n",
      "42731      5285\n",
      "5849       4318\n",
      "41401      4183\n",
      "25000      3738\n",
      "2724       3259\n",
      "51881      3081\n",
      "5990       2961\n",
      "53081      2623\n",
      "2449       2230\n",
      "486        2199\n",
      "2859       2186\n",
      "496        2051\n",
      "2720       2042\n",
      "2762       1915\n",
      "V4581      1820\n",
      "V5861      1819\n",
      "99592      1796\n",
      "2851       1756\n"
     ]
    }
   ],
   "source": [
    "print(diagnoses.groupby(\"ICD9_CODE\").size().to_frame(\"diagnosis_freq_by_icd9_code\").describe())\n",
    "print(diagnoses.groupby(\"SUBJECT_ID\").size().to_frame(\"diagnosis_freq_by_patient\").describe())\n",
    "print(diagnoses.groupby(\"HADM_ID\").size().to_frame(\"diagnosis_freq_by_admission\").describe())\n",
    "\n",
    "print(diagnoses.groupby(\"ICD9_CODE\").size().to_frame(\"freq\").sort_values(\"freq\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    739127.000000\n",
      "mean        280.506953\n",
      "std         381.865602\n",
      "min           0.000000\n",
      "25%          72.000000\n",
      "50%         160.000000\n",
      "75%         318.000000\n",
      "max        7980.000000\n",
      "Name: text_len, dtype: float64\n",
      "       notes_freq_by_patient\n",
      "count            7535.000000\n",
      "mean               98.092502\n",
      "std               117.930373\n",
      "min                 1.000000\n",
      "25%                34.000000\n",
      "50%                62.000000\n",
      "75%               116.000000\n",
      "max              1420.000000\n",
      "       notes_freq_by_admission\n",
      "count             19758.000000\n",
      "mean                 37.408999\n",
      "std                 948.520727\n",
      "min                   1.000000\n",
      "25%                   8.000000\n",
      "50%                  15.000000\n",
      "75%                  31.000000\n",
      "max              133139.000000\n"
     ]
    }
   ],
   "source": [
    "notes['text_len'] = notes['TEXT'].apply(lambda s: len(s.split()))\n",
    "print(notes[\"text_len\"].describe())\n",
    "print(notes.groupby(\"SUBJECT_ID\").size().to_frame(\"notes_freq_by_patient\").describe())\n",
    "print(notes.groupby(\"HADM_ID\").size().to_frame(\"notes_freq_by_admission\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    21740.000000\n",
      "mean      1603.497148\n",
      "std        884.371393\n",
      "min          9.000000\n",
      "25%       1008.000000\n",
      "50%       1527.000000\n",
      "75%       2111.000000\n",
      "max       7980.000000\n",
      "Name: text_len, dtype: float64\n",
      "       notes_freq_by_patient\n",
      "count            7451.000000\n",
      "mean                2.917729\n",
      "std                 1.935627\n",
      "min                 1.000000\n",
      "25%                 2.000000\n",
      "50%                 2.000000\n",
      "75%                 3.000000\n",
      "max                47.000000\n",
      "       notes_freq_by_admission\n",
      "count             19050.000000\n",
      "mean                  1.141207\n",
      "std                   0.438223\n",
      "min                   1.000000\n",
      "25%                   1.000000\n",
      "50%                   1.000000\n",
      "75%                   1.000000\n",
      "max                   7.000000\n"
     ]
    }
   ],
   "source": [
    "discharge_summaries = notes[notes.CATEGORY == \"Discharge summary\"]\n",
    "print(discharge_summaries[\"text_len\"].describe())\n",
    "print(discharge_summaries.groupby(\"SUBJECT_ID\").size().to_frame(\"notes_freq_by_patient\").describe())\n",
    "print(discharge_summaries.groupby(\"HADM_ID\").size().to_frame(\"notes_freq_by_admission\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3902.000000\n",
      "mean      663.113275\n",
      "std       811.485606\n",
      "min         0.000000\n",
      "25%        80.250000\n",
      "50%       316.500000\n",
      "75%       937.750000\n",
      "max      4328.000000\n",
      "Name: death_interval, dtype: float64\n",
      "Prevalence of death in 30 days =  0.05532705320419265\n",
      "Prevalence of death =  0.5177126177524214\n"
     ]
    }
   ],
   "source": [
    "patients[\"death_date\"] = patients[\"DOD\"].apply(lambda s: (datetime.datetime.strptime(s, '%Y-%m-%d %H:%M:%S') - datetime.datetime(1970,1,1)).days if s != '' else np.nan)\n",
    "patients['death_interval'] = patients.death_date - patients.record_end_date\n",
    "print(patients['death_interval'].describe())\n",
    "print(\"Prevalence of death in 30 days = \", len(patients[patients.death_interval < 30]) / len(patients))\n",
    "print(\"Prevalence of death = \", len(patients[patients.death_interval >= 0]) / len(patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       procedure_freq_by_icd9_code\n",
      "count                    16.000000\n",
      "mean                   2182.312500\n",
      "std                    1401.472546\n",
      "min                     982.000000\n",
      "25%                    1333.250000\n",
      "50%                    1558.500000\n",
      "75%                    2832.000000\n",
      "max                    6505.000000\n",
      "       procedure_freq_by_patient\n",
      "count                6459.000000\n",
      "mean                    5.405945\n",
      "std                     4.799150\n",
      "min                     1.000000\n",
      "25%                     2.000000\n",
      "50%                     4.000000\n",
      "75%                     7.000000\n",
      "max                    50.000000\n",
      "       procedure_freq_by_admission\n",
      "count                 13165.000000\n",
      "mean                      2.652260\n",
      "std                       1.987886\n",
      "min                       1.000000\n",
      "25%                       1.000000\n",
      "50%                       2.000000\n",
      "75%                       3.000000\n",
      "max                      27.000000\n"
     ]
    }
   ],
   "source": [
    "# Limit the procedures to the most common procedures\n",
    "NUM_PROCEDURE_CODES = 16\n",
    "top_procedures = procedures.groupby(\"ICD9_CODE\").size().to_frame(\"freq\").sort_values(\"freq\", ascending=False).head(NUM_PROCEDURE_CODES).index.tolist()\n",
    "procedures = procedures[procedures.ICD9_CODE.isin(top_procedures)]\n",
    "print(procedures.groupby(\"ICD9_CODE\").size().to_frame(\"procedure_freq_by_icd9_code\").describe())\n",
    "print(procedures.groupby(\"SUBJECT_ID\").size().to_frame(\"procedure_freq_by_patient\").describe())\n",
    "print(procedures.groupby(\"HADM_ID\").size().to_frame(\"procedure_freq_by_admission\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       diagnosis_freq_by_icd9_code\n",
      "count                    16.000000\n",
      "mean                   3490.125000\n",
      "std                    1643.981625\n",
      "min                    1915.000000\n",
      "25%                    2195.750000\n",
      "50%                    3021.000000\n",
      "75%                    4216.750000\n",
      "max                    7183.000000\n",
      "       diagnosis_freq_by_patient\n",
      "count                7008.000000\n",
      "mean                    7.968322\n",
      "std                     6.438043\n",
      "min                     1.000000\n",
      "25%                     4.000000\n",
      "50%                     7.000000\n",
      "75%                    10.000000\n",
      "max                   102.000000\n",
      "       diagnosis_freq_by_admission\n",
      "count                 17466.000000\n",
      "mean                      3.197183\n",
      "std                       1.733602\n",
      "min                       1.000000\n",
      "25%                       2.000000\n",
      "50%                       3.000000\n",
      "75%                       4.000000\n",
      "max                      12.000000\n"
     ]
    }
   ],
   "source": [
    "# Limit the diagnoses to the most common diagnoses\n",
    "NUM_DIAGNOSIS_CODES = 16\n",
    "top_diagnoses = diagnoses.groupby(\"ICD9_CODE\").size().to_frame(\"freq\").sort_values(\"freq\", ascending=False).head(NUM_DIAGNOSIS_CODES).index.tolist()\n",
    "diagnoses = diagnoses[diagnoses.ICD9_CODE.isin(top_diagnoses)]\n",
    "print(diagnoses.groupby(\"ICD9_CODE\").size().to_frame(\"diagnosis_freq_by_icd9_code\").describe())\n",
    "print(diagnoses.groupby(\"SUBJECT_ID\").size().to_frame(\"diagnosis_freq_by_patient\").describe())\n",
    "print(diagnoses.groupby(\"HADM_ID\").size().to_frame(\"diagnosis_freq_by_admission\").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7537\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "admission_encoding_size = len(top_procedures) + len(top_diagnoses) + 1\n",
    "# encode each admission as\n",
    "#   length of stay\n",
    "#   frequency of each procedure in top_procedures\n",
    "#   frequency of each diagnosis in top_diagnoses\n",
    "#   TODO: add the embedding of the clinical notes here\n",
    "                \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, patients, admissions, procedures, top_procedures, diagnoses, top_diagnoses, prediction_window):\n",
    "        top_procedures_dict = dict(zip(top_procedures, range(len(top_procedures))))\n",
    "        top_diagnoses_dict = dict(zip(top_diagnoses, range(len(top_diagnoses))))\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for _, patient in patients.iterrows():\n",
    "            patient_admissions = []\n",
    "            for _, admission in admissions[admissions.SUBJECT_ID == patient.SUBJECT_ID].iterrows():\n",
    "                patient_admission = np.zeros(admission_encoding_size)\n",
    "                patient_admission[0] = admission.LOS\n",
    "                for _, admission_procedure in procedures[procedures.HADM_ID == admission.HADM_ID].iterrows():\n",
    "                    patient_admission[1 + top_procedures_dict[admission_procedure.ICD9_CODE]] += 1\n",
    "                for _, admission_diagnosis in diagnoses[diagnoses.HADM_ID == admission.HADM_ID].iterrows():\n",
    "                    patient_admission[1 + len(top_procedures) + top_diagnoses_dict[admission_diagnosis.ICD9_CODE]] += 1\n",
    "                patient_admissions.append(patient_admission)\n",
    "            self.x.append(patient_admissions)\n",
    "            # self.y.append([patient.final_admission_interval < prediction_window, patient.death_interval < prediction_window])\n",
    "            self.y.append(patient.final_admission_interval < prediction_window)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(patients, previous_admissions, procedures, top_procedures, diagnoses, top_diagnoses, 30)\n",
    "print(len(dataset))\n",
    "# for i in range(len(dataset)):\n",
    "#     x, y = dataset[i]\n",
    "#     print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, encoding size). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (# patiens, max # visits, encoding size) of type torch.long\n",
    "        masks: a tensor of shape (# patiens, max # visits, encoding size) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (# patiens) of type torch.float\n",
    "    \"\"\"\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "    num_patients = len(sequences)\n",
    "    max_admissions = 0\n",
    "    for sequence in sequences:\n",
    "        max_admissions = max(max_admissions, len(sequence))\n",
    "    \n",
    "    dim = (num_patients, max_admissions, admission_encoding_size)\n",
    "    x_data = np.zeros(dim)\n",
    "    rev_x_data = np.zeros(dim)\n",
    "    masks_data = np.full(dim, False)\n",
    "    rev_masks_data = np.full(dim, False)\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        num_admissions = len(sequence)\n",
    "        for j, admission in enumerate(sequence):\n",
    "            x_data[i][j] = admission\n",
    "            rev_x_data[i][num_admissions-j-1] = admission\n",
    "            masks_data[i][j] = np.ones_like(masks_data[i][j])\n",
    "            rev_masks_data[i][num_admissions-j-1] = np.ones_like(rev_masks_data[i][j])\n",
    "\n",
    "    x = torch.tensor(x_data, dtype=torch.float)\n",
    "    rev_x = torch.tensor(rev_x_data, dtype=torch.float)\n",
    "    masks = torch.tensor(masks_data, dtype=torch.bool)\n",
    "    rev_masks = torch.tensor(rev_masks_data, dtype=torch.bool)\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 33]) torch.Size([10, 5, 33]) torch.Size([10, 5, 33]) torch.Size([10, 5, 33]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "print(x.shape, masks.shape, rev_x.shape, rev_masks.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6029\n",
      "Length of val dataset: 1508\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    batch_size, visits, embedding_dim = hidden_states.shape\n",
    "    masks = torch.sum(masks, 2)\n",
    "    masks = torch.min(masks, torch.ones_like(masks))\n",
    "    masks = torch.sum(masks, 1)\n",
    "    masks = masks - torch.ones_like(masks)\n",
    "    masks = masks.unsqueeze(1).expand(batch_size, embedding_dim).unsqueeze(1)\n",
    "    masks = torch.max(masks, torch.zeros_like(masks)) # FIXME: data cleaning problem! some patients have no admission\n",
    "    last_visit = torch.gather(hidden_states, 1, masks)\n",
    "    last_visit = torch.flatten(last_visit, 1, 2)\n",
    "    return last_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (rnn): GRU(33, 64, batch_first=True)\n",
       "  (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation1): ReLU()\n",
       "  (activation2): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveRNN(torch.nn.Module):\n",
    "    def __init__(self, admission_encoding_size, hidden_state_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.rnn = torch.nn.GRU(admission_encoding_size, hidden_state_size, batch_first=True)\n",
    "        self.linear1 = torch.nn.Linear(hidden_state_size, 32)\n",
    "        self.linear2 = torch.nn.Linear(32, output_size)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.activation2 = torch.nn.Sigmoid()\n",
    "    \n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        (batch_size, num_admissions, _) = x.shape\n",
    "        hidden_state = torch.zeros(1, batch_size, self.hidden_state_size)\n",
    "        hidden_states = []\n",
    "        for admission in range(num_admissions):\n",
    "            _, hidden_state = self.rnn(torch.narrow(x, 1, admission, 1), hidden_state)\n",
    "            hidden_states.append(hidden_state)\n",
    "        hidden_states = torch.cat(tuple(hidden_states), 0)\n",
    "        hidden_states = torch.transpose(hidden_states, 0, 1)\n",
    "        \n",
    "        output = get_last_visit(hidden_states, masks)\n",
    "        output = self.activation1(self.linear1(output))\n",
    "        output = self.dropout(output)\n",
    "        output = self.activation2(self.linear2(output))\n",
    "        return output.squeeze()\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "naive_rnn = NaiveRNN(admission_encoding_size=admission_encoding_size, hidden_state_size=64, output_size=1)\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    model.eval()\n",
    "    Y1_pred = []\n",
    "    Y1_true = []\n",
    "    # Y2_pred = []\n",
    "    # Y2_true = []\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        pred = model(x, masks, rev_x, rev_masks)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x, masks, rev_x, rev_masks)\n",
    "            Y1_true.extend(y.detach().numpy().tolist())\n",
    "            Y1_pred.extend(pred.detach().numpy().reshape(-1).tolist())\n",
    "            # Y1_true.extend(y.detach().numpy()[:,0].tolist())\n",
    "            # Y1_pred.extend(pred.detach().numpy()[:,0].reshape(-1).tolist())\n",
    "            # Y2_true.extend(y.detach().numpy()[:,1].tolist())\n",
    "            # Y2_pred.extend(pred.detach().numpy()[:,1].reshape(-1).tolist())\n",
    "    \n",
    "    precision1, recall1, f11, _ = precision_recall_fscore_support(Y1_true, np.array(Y1_pred)>0.5, average='binary')\n",
    "    roc_auc1 = roc_auc_score(Y1_true, Y1_pred)\n",
    "    # precision2, recall2, f12, _ = precision_recall_fscore_support(Y2_true, np.array(Y2_pred)>0.5, average='binary')\n",
    "    # roc_auc2 = roc_auc_score(Y2_true, Y2_pred)\n",
    "    \n",
    "    return precision1, recall1, f11, roc_auc1# , precision2, recall2, f12, roc_auc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task1: P=0.418 R=0.229 F1=0.296 ROC AUC=0.606\n"
     ]
    }
   ],
   "source": [
    "precision1, recall1, f11, roc_auc1 = eval_model(naive_rnn, val_loader)\n",
    "print('Task1: P={:.3f} R={:.3f} F1={:.3f} ROC AUC={:.3f}'.format(precision1, recall1, f11, roc_auc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tTask1: P=0.439 R=0.259 F1=0.326 ROC AUC=0.603\n",
      "\n",
      "Epoch 1\n",
      "\tTask1: P=0.429 R=0.216 F1=0.288 ROC AUC=0.598\n",
      "\n",
      "Epoch 2\n",
      "\tTask1: P=0.448 R=0.266 F1=0.334 ROC AUC=0.602\n",
      "\n",
      "Epoch 3\n",
      "\tTask1: P=0.428 R=0.294 F1=0.348 ROC AUC=0.601\n",
      "\n",
      "Epoch 4\n",
      "\tTask1: P=0.427 R=0.279 F1=0.337 ROC AUC=0.595\n",
      "\n",
      "Epoch 5\n",
      "\tTask1: P=0.448 R=0.266 F1=0.334 ROC AUC=0.605\n",
      "\n",
      "Epoch 6\n",
      "\tTask1: P=0.426 R=0.249 F1=0.314 ROC AUC=0.595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x, masks, rev_x, rev_masks)\n",
    "            loss = criterion(y_pred, y)\n",
    "            # y_task1 = torch.narrow(y, 1, 0, 1)\n",
    "            # y_pred_task1 = torch.narrow(y_pred, 1, 0, 1)\n",
    "            # loss = criterion(y_pred_task1, y_task1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # precision1, recall1, f11, roc_auc1, precision2, recall2, f12, roc_auc2 = eval_model(model, val_loader)\n",
    "        # print('Epoch {}\\n\\tTask1: P={:.3f} R={:.3f} F1={:.3f} ROC AUC={:.3f}\\n\\tTask2: P={:.3f} R={:.3f} F1={:.3f} ROC AUC={:.3f}'.format(epoch, precision1, recall1, f11, roc_auc1, precision2, recall2, f12, roc_auc2))\n",
    "        precision1, recall1, f11, roc_auc1 = eval_model(model, val_loader)\n",
    "        print('Epoch {}\\n\\tTask1: P={:.3f} R={:.3f} F1={:.3f} ROC AUC={:.3f}\\n'.format(epoch, precision1, recall1, f11, roc_auc1))\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
