{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "FILE_NAME_SUFFIX = \".multi_admission\" \n",
    "# FILE_NAME_SUFFIX = \"_1000\"\n",
    "# FILE_NAME_SUFFIX = \"\" \n",
    "DATASET_PATH = \"Datasets/\"\n",
    "\n",
    "# !ls $DATASET_PATH\n",
    "\n",
    "def read_samples(input_file, file_name_suffix):\n",
    "    file_name = DATASET_PATH + input_file + file_name_suffix\n",
    "    return pd.read_csv(file_name, error_bad_lines=False, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = read_samples(\"PATIENTS.csv\", FILE_NAME_SUFFIX)\n",
    "admissions = read_samples(\"ADMISSIONS.csv\", FILE_NAME_SUFFIX)\n",
    "diagnoses = read_samples(\"DIAGNOSES_ICD.csv\", FILE_NAME_SUFFIX)\n",
    "icu_stays = read_samples(\"ICUSTAYS.csv\", FILE_NAME_SUFFIX)\n",
    "procedures = read_samples(\"PROCEDURES_ICD.csv\", FILE_NAME_SUFFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Patients:\")\n",
    "# print(patients.head(5))\n",
    "# print(\"Admissions:\")\n",
    "# print(admissions.head(5))\n",
    "# print(\"Diagnoses:\")\n",
    "# print(diagnoses.head(5))\n",
    "# print(\"ICU stays:\")\n",
    "# print(icu_stays.head(5))\n",
    "# print(\"Procedures:\")\n",
    "# print(procedures.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = patients.set_index(\"SUBJECT_ID\", drop=False)\n",
    "patients[\"num_admissions\"] = admissions.groupby(\"SUBJECT_ID\").size().to_frame(\"num_admissions\")\n",
    "patients = patients[patients.num_admissions > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = admissions[admissions.SUBJECT_ID.isin(patients.SUBJECT_ID)]\n",
    "procedures = procedures[procedures.SUBJECT_ID.isin(patients.SUBJECT_ID)]\n",
    "diagnoses = diagnoses[diagnoses.SUBJECT_ID.isin(patients.SUBJECT_ID)]\n",
    "# icu_stays = icu_stays[icu_stays.SUBJECT_ID.isin(patients.SUBJECT_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_admission = admissions[admissions.groupby(['SUBJECT_ID'])['ADMITTIME'].transform(max) == admissions['ADMITTIME']]\n",
    "# print(last_admission.size)\n",
    "# print(admissions.size)\n",
    "previous_admissions = admissions[admissions.groupby(['SUBJECT_ID'])['ADMITTIME'].transform(max) != admissions['ADMITTIME']]\n",
    "# print(previous_admissions.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = []\n",
    "morts = []\n",
    "types = {}\n",
    "for patient in patients.itertuples():\n",
    "#     print(patient.ROW_ID)\n",
    "#     print(patient.SUBJECT_ID)\n",
    "    patient_admissions = []\n",
    "    for patient_admission in previous_admissions[previous_admissions.SUBJECT_ID == patient.SUBJECT_ID].itertuples():\n",
    "        icd9_codes = []\n",
    "#         print(patient_admission.HADM_ID)\n",
    "#         newdf = df[(df.origin == \"JFK\") & (df.carrier == \"B6\")]\n",
    "        diagnoses_filtered = diagnoses[(diagnoses.SUBJECT_ID == patient.SUBJECT_ID) & (diagnoses.HADM_ID == patient_admission.HADM_ID)]\n",
    "        procedures_filtered = procedures[(procedures.SUBJECT_ID == patient.SUBJECT_ID) & (procedures.HADM_ID == patient_admission.HADM_ID)]\n",
    "#         print(diagnoses_filtered)\n",
    "        for admission_diagnosis in diagnoses_filtered.itertuples():\n",
    "            if admission_diagnosis.ICD9_CODE in types:\n",
    "                icd9_codes.append(types[admission_diagnosis.ICD9_CODE])\n",
    "            else:\n",
    "                types[admission_diagnosis.ICD9_CODE] = len(types)\n",
    "                icd9_codes.append(types[admission_diagnosis.ICD9_CODE])\n",
    "        for admission_procedures in procedures_filtered.itertuples():\n",
    "            if admission_procedures.ICD9_CODE in types:\n",
    "                icd9_codes.append(types[admission_procedures.ICD9_CODE])\n",
    "            else:\n",
    "                types[admission_procedures.ICD9_CODE] = len(types)\n",
    "                icd9_codes.append(types[admission_procedures.ICD9_CODE])\n",
    "#             print(admission_diagnosis.SEQ_NUM)\n",
    "#             print(icd9_codes)\n",
    "        patient_admissions.append(icd9_codes)\n",
    "    morts.append(patient.EXPIRE_FLAG)\n",
    "    seqs.append(patient_admissions)\n",
    "# print(patient_morts)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array(seq).shape)\n",
    "# print(seqs)\n",
    "# print(\"Mortality:\", morts[335])\n",
    "# for visit in range(len(seqs[335])):\n",
    "#     print(f\"\\t{visit}-th admission diagnosis labels:\", seqs[335][visit])\n",
    "# print(f\"admission diagnosis labels:\", seqs[335])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, morts):\n",
    "        \n",
    "        self.x = seqs\n",
    "        self.y = morts\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        #Tuple of (seq,label) where seq is seq[i][j][k] and label is mortality\n",
    "        label = self.y[index]\n",
    "        sequence = self.x[index]\n",
    "        return(sequence,label)\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(seqs, morts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sequences, labels = zip(*data)\n",
    "    import copy\n",
    "\n",
    "    max_visits = 0\n",
    "    max_codes = 0\n",
    "    x = []\n",
    "    masks = []\n",
    "    \n",
    "    for patient_visits in sequences:\n",
    "        max_visits = max(len(patient_visits),max_visits)\n",
    "        for patient_visit_codes in patient_visits:\n",
    "            max_codes = max(len(patient_visit_codes),max_codes)\n",
    "            \n",
    "#     print(max_visits)\n",
    "#     print(max_codes) \n",
    "\n",
    "    for patient_visits in sequences:\n",
    "        \n",
    "        patient_masks = []\n",
    "        patient_visits_c = []\n",
    "        \n",
    "        for patient_single_visit_codes in patient_visits:\n",
    "            mask = [1]*len(patient_single_visit_codes)\n",
    "            patient_single_visit_codes_c = copy.deepcopy(patient_single_visit_codes)\n",
    "#             print(\"mask before\")\n",
    "#             print(mask)\n",
    "            if len(patient_single_visit_codes) < max_codes:\n",
    "                padding = max_codes - len(patient_single_visit_codes)\n",
    "#                 print(patient_single_visit_codes)\n",
    "                patient_single_visit_codes_c += [0] * padding\n",
    "                mask += [0] * padding\n",
    "#             print(\"mask after\")\n",
    "#             print(mask)\n",
    "#             print(print(patient_single_visit_codes))\n",
    "            patient_visits_c.append(patient_single_visit_codes_c)\n",
    "            patient_masks.append(mask)\n",
    "#                 print(patient_visit_codes)   \n",
    "#             print(\"patient_masks\")\n",
    "#             print(patient_masks)\n",
    "        \n",
    "#         print(patient_visits)\n",
    "        \n",
    "        if len(patient_visits) < max_visits:\n",
    "            for i in range (0, (max_visits - len(patient_visits))):        \n",
    "                patient_visits_c.append(([0] * max_codes))\n",
    "                patient_masks.append(([0] * max_codes))                \n",
    "#         print(patient_visits)\n",
    "\n",
    "        x.append(patient_visits_c)\n",
    "        masks.append(patient_masks)\n",
    "    \n",
    "#     print(\"masks\") \n",
    "#     print(masks)\n",
    "#     print(\"x\")\n",
    "#     print(x)\n",
    "#     print(sequences)\n",
    "    x = torch.Tensor(x).long()\n",
    "    masks = torch.Tensor(masks).bool()\n",
    "    \n",
    "    y = torch.Tensor(labels).float()\n",
    "          \n",
    "    return x, masks, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 6029\n",
      "Length of val dataset: 1508\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)\n",
    "# print(len(train_loader))\n",
    "# print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the embeddings for true visits (not padding visits) and then\n",
    "        sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "#     print(x[1])\n",
    "#     print(masks[1])\n",
    "    x_copy = x.clone()\n",
    "    x_copy[masks==False] = 0\n",
    "    sum_embeddings = torch.sum(x_copy,2)\n",
    "#     print(sum_embeddings)\n",
    "    return sum_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    batch_size, visits, embedding_dim = hidden_states.shape\n",
    "    masks = torch.sum(masks, 2)\n",
    "    masks = torch.min(masks, torch.ones_like(masks))\n",
    "    masks = torch.sum(masks, 1)\n",
    "    masks = masks - torch.ones_like(masks)\n",
    "    masks = masks.unsqueeze(1).expand(batch_size, embedding_dim).unsqueeze(1)\n",
    "    masks = torch.max(masks, torch.zeros_like(masks)) \n",
    "    last_visit = torch.gather(hidden_states, 1, masks)\n",
    "    last_visit = torch.flatten(last_visit, 1, 2)\n",
    "    return last_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (em): Embedding(260326, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveRNN(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.em = nn.Embedding(num_embeddings = num_codes, embedding_dim = 128)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size = 128, hidden_size = 128, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(128, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, masks):\n",
    "\n",
    "        x = self.em(x)\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        x , _ = self.rnn(x)\n",
    "        x = get_last_visit(x, masks)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = x.view(-1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "naive_rnn = NaiveRNN(num_codes = len(diagnoses))\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_probs = []\n",
    "    \n",
    "    for step, batch in enumerate(val_loader):\n",
    "        x, masks, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            probs = model(x, masks)\n",
    "            val_labels.extend(labels.detach().tolist())\n",
    "            val_probs.extend(probs.detach().numpy().reshape(-1).tolist())\n",
    "            \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, np.array(val_probs) > 0.5, average='binary')\n",
    "    roc_auc = roc_auc_score(val_labels, val_probs)\n",
    "    print(sum(val_labels))\n",
    "    print(len(val_labels))\n",
    "    \n",
    "    print(f\"roc_auc:{roc_auc:3f}, precision:{precision:.3f},recall:{recall:3f},f1:{f1:3f}\")\n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788.0\n",
      "1508\n",
      "roc_auc:0.716950, precision:0.663,recall:0.691624,f1:0.677019\n",
      "788.0\n",
      "1508\n",
      "roc_auc:0.723858, precision:0.651,recall:0.717005,f1:0.682367\n",
      "788.0\n",
      "1508\n",
      "roc_auc:0.730831, precision:0.666,recall:0.706853,f1:0.685961\n",
      "788.0\n",
      "1508\n",
      "roc_auc:0.725130, precision:0.667,recall:0.695431,f1:0.680745\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d415abefaae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# number of epochs to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-d415abefaae2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, n_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pioneers/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pioneers/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            x, masks, labels = batch\n",
    "            \n",
    "            y_hat = model.forward(x, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(y_hat, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "            \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "#         print(train_loss)\n",
    "        eval_model(model, val_loader)\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
