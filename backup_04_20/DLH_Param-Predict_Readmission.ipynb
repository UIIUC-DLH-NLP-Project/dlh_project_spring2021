{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# FILE_NAME_SUFFIX = \".multi_admission\" \n",
    "FILE_NAME_SUFFIX = \"_1000\"\n",
    "# FILE_NAME_SUFFIX = \"\" \n",
    "DATASET_PATH = \"Datasets/\"\n",
    "\n",
    "# !ls $DATASET_PATH\n",
    "\n",
    "def read_samples(input_file, file_name_suffix):\n",
    "    file_name = DATASET_PATH + input_file + file_name_suffix\n",
    "    return pd.read_csv(file_name, error_bad_lines=False, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = read_samples(\"PATIENTS.csv\", FILE_NAME_SUFFIX)\n",
    "admissions = read_samples(\"ADMISSIONS.csv\", FILE_NAME_SUFFIX)\n",
    "diagnoses = read_samples(\"DIAGNOSES_ICD.csv\", FILE_NAME_SUFFIX)\n",
    "icu_stays = read_samples(\"ICUSTAYS.csv\", FILE_NAME_SUFFIX)\n",
    "procedures = read_samples(\"PROCEDURES_ICD.csv\", FILE_NAME_SUFFIX)\n",
    "notes = read_samples(\"NOTEEVENTS.csv\", FILE_NAME_SUFFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Patients:\")\n",
    "# print(patients.head(5))\n",
    "# print(\"Admissions:\")\n",
    "# print(admissions.head(5))\n",
    "# print(\"Diagnoses:\")\n",
    "# print(diagnoses.head(5))\n",
    "# print(\"ICU stays:\")\n",
    "# print(icu_stays.head(5))\n",
    "# print(\"Procedures:\")\n",
    "# print(procedures.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADMISSION_TYPE\n",
      "ELECTIVE     137\n",
      "EMERGENCY    845\n",
      "NEWBORN      220\n",
      "URGENT        40\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(admissions.groupby('ADMISSION_TYPE').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = admissions[admissions['ADMISSION_TYPE']!='NEWBORN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# admissions.ADMITTIME = pd.to_datetime(admissions.ADMITTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "admissions['ADMIT_DATE'] = admissions.ADMITTIME.apply(lambda x: str(x).split(' ')[0])\n",
    "admissions['ADMIT_DATE'] = pd.to_datetime(admissions.ADMIT_DATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "# admissions.DISCHTIME = pd.to_datetime(admissions.DISCHTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "admissions['DISCH_DATE'] = admissions.DISCHTIME.apply(lambda x: str(x).split(' ')[0])\n",
    "admissions['DISCH_DATE'] = pd.to_datetime(admissions.DISCH_DATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "admissions.DEATHTIME = pd.to_datetime(admissions.DEATHTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SUBJECT_ID            ADMITTIME ADMIT_DATE            DISCHTIME  \\\n",
      "0             22  2196-04-09 12:26:00 2196-04-09  2196-04-10 15:54:00   \n",
      "1             23  2153-09-03 07:15:00 2153-09-03  2153-09-08 19:10:00   \n",
      "2             23  2157-10-18 19:34:00 2157-10-18  2157-10-25 14:00:00   \n",
      "3             24  2139-06-06 16:14:00 2139-06-06  2139-06-09 12:48:00   \n",
      "4             25  2160-11-02 02:06:00 2160-11-02  2160-11-05 14:55:00   \n",
      "...          ...                  ...        ...                  ...   \n",
      "1231         961  2101-11-13 00:41:00 2101-11-13  2101-11-24 15:00:00   \n",
      "1233         963  2195-04-24 14:50:00 2195-04-24  2195-04-30 13:27:00   \n",
      "1234         963  2204-10-25 16:42:00 2204-10-25  2204-11-04 13:55:00   \n",
      "1240         969  2151-09-23 14:35:00 2151-09-23  2151-09-30 16:03:00   \n",
      "1241         969  2162-05-03 15:31:00 2162-05-03  2162-05-11 14:24:00   \n",
      "\n",
      "     DISCH_DATE  \n",
      "0    2196-04-10  \n",
      "1    2153-09-08  \n",
      "2    2157-10-25  \n",
      "3    2139-06-09  \n",
      "4    2160-11-05  \n",
      "...         ...  \n",
      "1231 2101-11-24  \n",
      "1233 2195-04-30  \n",
      "1234 2204-11-04  \n",
      "1240 2151-09-30  \n",
      "1241 2162-05-11  \n",
      "\n",
      "[1022 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(admissions[['SUBJECT_ID','ADMITTIME','ADMIT_DATE','DISCHTIME','DISCH_DATE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = admissions.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "admissions = admissions.reset_index(drop = True)\n",
    "admissions['NEXT_ADMIT_DATE'] = admissions.groupby('SUBJECT_ID').ADMIT_DATE.shift(-1)\n",
    "admissions['NEXT_ADMISSION_TYPE'] = admissions.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SUBJECT_ID            ADMITTIME NEXT_ADMIT_DATE NEXT_ADMISSION_TYPE\n",
      "0              3  2101-10-20 19:08:00             NaT                 NaN\n",
      "1              4  2191-03-16 00:28:00             NaT                 NaN\n",
      "2              6  2175-05-30 07:15:00             NaT                 NaN\n",
      "3              9  2149-11-09 13:06:00             NaT                 NaN\n",
      "4             11  2178-04-16 06:18:00             NaT                 NaN\n",
      "...          ...                  ...             ...                 ...\n",
      "1017         998  2152-06-26 16:22:00      2153-09-05            ELECTIVE\n",
      "1018         998  2153-09-05 09:00:00      2153-10-07           EMERGENCY\n",
      "1019         998  2153-10-07 13:57:00             NaT                 NaN\n",
      "1020         999  2119-06-04 21:36:00             NaT                 NaN\n",
      "1021        1000  2144-01-19 20:15:00             NaT                 NaN\n",
      "\n",
      "[1022 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(admissions[['SUBJECT_ID','ADMITTIME','NEXT_ADMIT_DATE','NEXT_ADMISSION_TYPE']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = admissions.NEXT_ADMISSION_TYPE == 'ELECTIVE'\n",
    "admissions.loc[rows,'NEXT_ADMIT_DATE'] = pd.NaT\n",
    "admissions.loc[rows,'NEXT_ADMISSION_TYPE'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = admissions.sort_values(['SUBJECT_ID','ADMITTIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back fill for a patient for which we removed the next admission as ELECTIVE\n",
    "admissions[['NEXT_ADMIT_DATE','NEXT_ADMISSION_TYPE']] = admissions.groupby(['SUBJECT_ID'])[['NEXT_ADMIT_DATE','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate days until next admission\n",
    "admissions['DAYS_NEXT_ADMIT'] = (admissions.NEXT_ADMIT_DATE - admissions.DISCH_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SUBJECT_ID DISCH_DATE NEXT_ADMIT_DATE DAYS_NEXT_ADMIT\n",
      "0              3 2101-10-31             NaT             NaT\n",
      "1              4 2191-03-23             NaT             NaT\n",
      "2              6 2175-06-15             NaT             NaT\n",
      "3              9 2149-11-14             NaT             NaT\n",
      "4             11 2178-05-11             NaT             NaT\n",
      "...          ...        ...             ...             ...\n",
      "1017         998 2152-06-30      2153-10-07        464 days\n",
      "1018         998 2153-09-18      2153-10-07         19 days\n",
      "1019         998 2153-10-23             NaT             NaT\n",
      "1020         999 2119-06-15             NaT             NaT\n",
      "1021        1000 2144-02-25             NaT             NaT\n",
      "\n",
      "[1022 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(admissions[['SUBJECT_ID','DISCH_DATE','NEXT_ADMIT_DATE','DAYS_NEXT_ADMIT']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions['OUTPUT_LABEL'] = (admissions.DAYS_NEXT_ADMIT < pd.Timedelta(days=30)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SUBJECT_ID DAYS_NEXT_ADMIT  OUTPUT_LABEL\n",
      "0              3             NaT             0\n",
      "1              4             NaT             0\n",
      "2              6             NaT             0\n",
      "3              9             NaT             0\n",
      "4             11             NaT             0\n",
      "...          ...             ...           ...\n",
      "1017         998        464 days             0\n",
      "1018         998         19 days             1\n",
      "1019         998             NaT             0\n",
      "1020         999             NaT             0\n",
      "1021        1000             NaT             0\n",
      "\n",
      "[1022 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(admissions[['SUBJECT_ID','DAYS_NEXT_ADMIT','OUTPUT_LABEL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions['DURATION'] = (admissions['DISCH_DATE']- admissions['ADMIT_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SUBJECT_ID ADMIT_DATE DISCH_DATE DURATION\n",
      "0              3 2101-10-20 2101-10-31  11 days\n",
      "1              4 2191-03-16 2191-03-23   7 days\n",
      "2              6 2175-05-30 2175-06-15  16 days\n",
      "3              9 2149-11-09 2149-11-14   5 days\n",
      "4             11 2178-04-16 2178-05-11  25 days\n",
      "...          ...        ...        ...      ...\n",
      "1017         998 2152-06-26 2152-06-30   4 days\n",
      "1018         998 2153-09-05 2153-09-18  13 days\n",
      "1019         998 2153-10-07 2153-10-23  16 days\n",
      "1020         999 2119-06-04 2119-06-15  11 days\n",
      "1021        1000 2144-01-19 2144-02-25  37 days\n",
      "\n",
      "[1022 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(admissions[['SUBJECT_ID','ADMIT_DATE','DISCH_DATE','DURATION']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# notes = notes[notes.HADM_ID.isin(admissions.HADM_ID)]\n",
    "# notes['HADM_ID'] = notes['HADM_ID'].astype(int)\n",
    "# notes = notes.sort_values(by=['SUBJECT_ID','HADM_ID','CHARTDATE'])\n",
    "# notes['CHARTDATE'] = pd.to_datetime(notes.CHARTDATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "# print(notes.dtypes)\n",
    "# print(admissions.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# admissions_notes = pd.merge(admissions[['SUBJECT_ID','HADM_ID','ADMIT_DATE','DISCH_DATE','DAYS_NEXT_ADMIT','NEXT_ADMIT_DATE','ADMISSION_TYPE','DEATHTIME','OUTPUT_LABEL','DURATION']],\n",
    "#                         notes[['SUBJECT_ID','HADM_ID','CHARTDATE','TEXT','CATEGORY']],\n",
    "#                         on = ['SUBJECT_ID','HADM_ID'],\n",
    "#                         how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       ROW_ID  SUBJECT_ID  HADM_ID            ADMITTIME            DISCHTIME  \\\n",
      "0          2           3   145834  2101-10-20 19:08:00  2101-10-31 13:58:00   \n",
      "1          3           4   185777  2191-03-16 00:28:00  2191-03-23 18:41:00   \n",
      "2          5           6   107064  2175-05-30 07:15:00  2175-06-15 16:00:00   \n",
      "3          8           9   150750  2149-11-09 13:06:00  2149-11-14 10:15:00   \n",
      "4         10          11   194540  2178-04-16 06:18:00  2178-05-11 19:00:00   \n",
      "...      ...         ...      ...                  ...                  ...   \n",
      "1017    1238         998   166191  2152-06-26 16:22:00  2152-06-30 20:06:00   \n",
      "1018    1239         998   171544  2153-09-05 09:00:00  2153-09-18 15:30:00   \n",
      "1019    1240         998   149668  2153-10-07 13:57:00  2153-10-23 17:00:00   \n",
      "1020    1241         999   173415  2119-06-04 21:36:00  2119-06-15 11:25:00   \n",
      "1021    1242        1000   143040  2144-01-19 20:15:00  2144-02-25 06:05:00   \n",
      "\n",
      "               DEATHTIME ADMISSION_TYPE         ADMISSION_LOCATION  \\\n",
      "0                    NaT      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
      "1                    NaT      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
      "2                    NaT       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
      "3    2149-11-14 10:15:00      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
      "4                    NaT      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
      "...                  ...            ...                        ...   \n",
      "1017                 NaT      EMERGENCY  TRANSFER FROM HOSP/EXTRAM   \n",
      "1018                 NaT       ELECTIVE  PHYS REFERRAL/NORMAL DELI   \n",
      "1019                 NaT      EMERGENCY  PHYS REFERRAL/NORMAL DELI   \n",
      "1020                 NaT      EMERGENCY       EMERGENCY ROOM ADMIT   \n",
      "1021 2144-02-25 06:05:00         URGENT  TRANSFER FROM HOSP/EXTRAM   \n",
      "\n",
      "             DISCHARGE_LOCATION INSURANCE  ...  \\\n",
      "0                           SNF  Medicare  ...   \n",
      "1     HOME WITH HOME IV PROVIDR   Private  ...   \n",
      "2              HOME HEALTH CARE  Medicare  ...   \n",
      "3                  DEAD/EXPIRED  Medicaid  ...   \n",
      "4              HOME HEALTH CARE   Private  ...   \n",
      "...                         ...       ...  ...   \n",
      "1017                       HOME  Medicare  ...   \n",
      "1018           HOME HEALTH CARE  Medicare  ...   \n",
      "1019           HOME HEALTH CARE  Medicare  ...   \n",
      "1020                        SNF  Medicare  ...   \n",
      "1021               DEAD/EXPIRED   Private  ...   \n",
      "\n",
      "                                             DIAGNOSIS HOSPITAL_EXPIRE_FLAG  \\\n",
      "0                                          HYPOTENSION                    0   \n",
      "1                  FEVER,DEHYDRATION,FAILURE TO THRIVE                    0   \n",
      "2                            CHRONIC RENAL FAILURE/SDA                    0   \n",
      "3                                      HEMORRHAGIC CVA                    1   \n",
      "4                                           BRAIN MASS                    0   \n",
      "...                                                ...                  ...   \n",
      "1017  CONGESTIVE HEART FAILURE;CORONARY ARTERY DISEASE                    0   \n",
      "1018                         CHRONIC RENAL FAILURE/SDA                    0   \n",
      "1019              FLUID OVERLOAD;S/P KIDNEY TRANSPLANT                    0   \n",
      "1020                                        CELLULITIS                    0   \n",
      "1021                                     HEART FAILURE                    1   \n",
      "\n",
      "     HAS_CHARTEVENTS_DATA ADMIT_DATE DISCH_DATE NEXT_ADMIT_DATE  \\\n",
      "0                       1 2101-10-20 2101-10-31             NaT   \n",
      "1                       1 2191-03-16 2191-03-23             NaT   \n",
      "2                       1 2175-05-30 2175-06-15             NaT   \n",
      "3                       1 2149-11-09 2149-11-14             NaT   \n",
      "4                       1 2178-04-16 2178-05-11             NaT   \n",
      "...                   ...        ...        ...             ...   \n",
      "1017                    1 2152-06-26 2152-06-30      2153-10-07   \n",
      "1018                    1 2153-09-05 2153-09-18      2153-10-07   \n",
      "1019                    1 2153-10-07 2153-10-23             NaT   \n",
      "1020                    1 2119-06-04 2119-06-15             NaT   \n",
      "1021                    1 2144-01-19 2144-02-25             NaT   \n",
      "\n",
      "     NEXT_ADMISSION_TYPE  DAYS_NEXT_ADMIT  OUTPUT_LABEL DURATION  \n",
      "0                    NaN              NaT             0  11 days  \n",
      "1                    NaN              NaT             0   7 days  \n",
      "2                    NaN              NaT             0  16 days  \n",
      "3                    NaN              NaT             0   5 days  \n",
      "4                    NaN              NaT             0  25 days  \n",
      "...                  ...              ...           ...      ...  \n",
      "1017           EMERGENCY         464 days             0   4 days  \n",
      "1018           EMERGENCY          19 days             1  13 days  \n",
      "1019                 NaN              NaT             0  16 days  \n",
      "1020                 NaN              NaT             0  11 days  \n",
      "1021                 NaN              NaT             0  37 days  \n",
      "\n",
      "[1022 rows x 26 columns]>\n"
     ]
    }
   ],
   "source": [
    "print(admissions.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    936\n",
       "1     86\n",
       "Name: OUTPUT_LABEL, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admissions.OUTPUT_LABEL.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(admissions.groupby('SUBJECT_ID')['SUBJECT_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SUBJECT_ID  OUTPUT_LABEL\n",
      "0              3             0\n",
      "1              4             0\n",
      "2              6             0\n",
      "3              9             0\n",
      "4             11             0\n",
      "...          ...           ...\n",
      "1017         998             0\n",
      "1018         998             1\n",
      "1019         998             0\n",
      "1020         999             0\n",
      "1021        1000             0\n",
      "\n",
      "[1022 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "subject_labels = admissions[['SUBJECT_ID', 'OUTPUT_LABEL']]\n",
    "print(subject_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SUBJECT_ID  OUTPUT_LABEL\n",
      "0             3             0\n",
      "1             4             0\n",
      "2             6             0\n",
      "3             9             0\n",
      "4            11             0\n",
      "..          ...           ...\n",
      "725         994             0\n",
      "726         995             0\n",
      "727         998             1\n",
      "728         999             0\n",
      "729        1000             0\n",
      "\n",
      "[730 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "subject_labels = subject_labels.groupby('SUBJECT_ID')[['OUTPUT_LABEL']].sum().reset_index()\n",
    "print(subject_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SUBJECT_ID  OUTPUT_LABELS_SUMMED\n",
      "0             3                     0\n",
      "1             4                     0\n",
      "2             6                     0\n",
      "3             9                     0\n",
      "4            11                     0\n",
      "..          ...                   ...\n",
      "725         994                     0\n",
      "726         995                     0\n",
      "727         998                     1\n",
      "728         999                     0\n",
      "729        1000                     0\n",
      "\n",
      "[730 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "subject_labels.rename(columns={\"OUTPUT_LABEL\":\"OUTPUT_LABELS_SUMMED\"}, inplace=True)\n",
    "print(subject_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SUBJECT_ID  OUTPUT_LABELS_SUMMED  OUTPUT_LABEL\n",
      "0             3                     0             0\n",
      "1             4                     0             0\n",
      "2             6                     0             0\n",
      "3             9                     0             0\n",
      "4            11                     0             0\n",
      "..          ...                   ...           ...\n",
      "725         994                     0             0\n",
      "726         995                     0             0\n",
      "727         998                     1             1\n",
      "728         999                     0             0\n",
      "729        1000                     0             0\n",
      "\n",
      "[730 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "subject_labels['OUTPUT_LABEL'] = (subject_labels['OUTPUT_LABELS_SUMMED'] >= 1).astype(int)\n",
    "print(subject_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_labels.drop(columns=['OUTPUT_LABELS_SUMMED'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SUBJECT_ID  OUTPUT_LABEL\n",
      "0             3             0\n",
      "1             4             0\n",
      "2             6             0\n",
      "3             9             0\n",
      "4            11             0\n",
      "..          ...           ...\n",
      "725         994             0\n",
      "726         995             0\n",
      "727         998             1\n",
      "728         999             0\n",
      "729        1000             0\n",
      "\n",
      "[730 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(subject_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'previous_admissions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5fc52bf726b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     print(patient.SUBJECT_ID)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpatient_admissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpatient_admission\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprevious_admissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprevious_admissions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUBJECT_ID\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpatient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUBJECT_ID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0micd9_codes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         print(patient_admission.HADM_ID)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'previous_admissions' is not defined"
     ]
    }
   ],
   "source": [
    "seqs = []\n",
    "readmission = []\n",
    "types = {}\n",
    "for patient in patients.itertuples():\n",
    "#     print(patient.ROW_ID)\n",
    "#     print(patient.SUBJECT_ID)\n",
    "    patient_admissions = []\n",
    "    for patient_admission in previous_admissions[previous_admissions.SUBJECT_ID == patient.SUBJECT_ID].itertuples():\n",
    "        icd9_codes = []\n",
    "#         print(patient_admission.HADM_ID)\n",
    "#         newdf = df[(df.origin == \"JFK\") & (df.carrier == \"B6\")]\n",
    "        diagnoses_filtered = diagnoses[(diagnoses.SUBJECT_ID == patient.SUBJECT_ID) & (diagnoses.HADM_ID == patient_admission.HADM_ID)]\n",
    "        procedures_filtered = procedures[(procedures.SUBJECT_ID == patient.SUBJECT_ID) & (procedures.HADM_ID == patient_admission.HADM_ID)]\n",
    "#         print(diagnoses_filtered)\n",
    "        for admission_diagnosis in diagnoses_filtered.itertuples():\n",
    "            if admission_diagnosis.ICD9_CODE in types:\n",
    "                icd9_codes.append(types[admission_diagnosis.ICD9_CODE])\n",
    "            else:\n",
    "                types[admission_diagnosis.ICD9_CODE] = len(types)\n",
    "                icd9_codes.append(types[admission_diagnosis.ICD9_CODE])\n",
    "        for admission_procedures in procedures_filtered.itertuples():\n",
    "            if admission_procedures.ICD9_CODE in types:\n",
    "                icd9_codes.append(types[admission_procedures.ICD9_CODE])\n",
    "            else:\n",
    "                types[admission_procedures.ICD9_CODE] = len(types)\n",
    "                icd9_codes.append(types[admission_procedures.ICD9_CODE])\n",
    "#             print(admission_diagnosis.SEQ_NUM)\n",
    "#             print(icd9_codes)\n",
    "        patient_admissions.append(icd9_codes)\n",
    "    readmission.append(patient.EXPIRE_FLAG)\n",
    "    seqs.append(patient_admissions)\n",
    "# print(patient_morts)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array(seq).shape)\n",
    "# print(seqs)\n",
    "# print(\"Mortality:\", morts[335])\n",
    "# for visit in range(len(seqs[335])):\n",
    "#     print(f\"\\t{visit}-th admission diagnosis labels:\", seqs[335][visit])\n",
    "# print(f\"admission diagnosis labels:\", seqs[335])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, morts):\n",
    "        \n",
    "        self.x = seqs\n",
    "        self.y = morts\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        #Tuple of (seq,label) where seq is seq[i][j][k] and label is mortality\n",
    "        label = self.y[index]\n",
    "        sequence = self.x[index]\n",
    "        return(sequence,label)\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(seqs, morts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sequences, labels = zip(*data)\n",
    "    import copy\n",
    "\n",
    "    max_visits = 0\n",
    "    max_codes = 0\n",
    "    x = []\n",
    "    masks = []\n",
    "    \n",
    "    for patient_visits in sequences:\n",
    "        max_visits = max(len(patient_visits),max_visits)\n",
    "        for patient_visit_codes in patient_visits:\n",
    "            max_codes = max(len(patient_visit_codes),max_codes)\n",
    "            \n",
    "#     print(max_visits)\n",
    "#     print(max_codes) \n",
    "\n",
    "    for patient_visits in sequences:\n",
    "        \n",
    "        patient_masks = []\n",
    "        patient_visits_c = []\n",
    "        \n",
    "        for patient_single_visit_codes in patient_visits:\n",
    "            mask = [1]*len(patient_single_visit_codes)\n",
    "            patient_single_visit_codes_c = copy.deepcopy(patient_single_visit_codes)\n",
    "#             print(\"mask before\")\n",
    "#             print(mask)\n",
    "            if len(patient_single_visit_codes) < max_codes:\n",
    "                padding = max_codes - len(patient_single_visit_codes)\n",
    "#                 print(patient_single_visit_codes)\n",
    "                patient_single_visit_codes_c += [0] * padding\n",
    "                mask += [0] * padding\n",
    "#             print(\"mask after\")\n",
    "#             print(mask)\n",
    "#             print(print(patient_single_visit_codes))\n",
    "            patient_visits_c.append(patient_single_visit_codes_c)\n",
    "            patient_masks.append(mask)\n",
    "#                 print(patient_visit_codes)   \n",
    "#             print(\"patient_masks\")\n",
    "#             print(patient_masks)\n",
    "        \n",
    "#         print(patient_visits)\n",
    "        \n",
    "        if len(patient_visits) < max_visits:\n",
    "            for i in range (0, (max_visits - len(patient_visits))):        \n",
    "                patient_visits_c.append(([0] * max_codes))\n",
    "                patient_masks.append(([0] * max_codes))                \n",
    "#         print(patient_visits)\n",
    "\n",
    "        x.append(patient_visits_c)\n",
    "        masks.append(patient_masks)\n",
    "    \n",
    "#     print(\"masks\") \n",
    "#     print(masks)\n",
    "#     print(\"x\")\n",
    "#     print(x)\n",
    "#     print(sequences)\n",
    "    x = torch.Tensor(x).long()\n",
    "    masks = torch.Tensor(masks).bool()\n",
    "    \n",
    "    y = torch.Tensor(labels).float()\n",
    "          \n",
    "    return x, masks, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.8)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    # your code here\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_data(train_dataset, val_dataset, collate_fn)\n",
    "# print(len(train_loader))\n",
    "# print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the embeddings for true visits (not padding visits) and then\n",
    "        sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "#     print(x[1])\n",
    "#     print(masks[1])\n",
    "    x_copy = x.clone()\n",
    "    x_copy[masks==False] = 0\n",
    "    sum_embeddings = torch.sum(x_copy,2)\n",
    "#     print(sum_embeddings)\n",
    "    return sum_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    batch_size, visits, embedding_dim = hidden_states.shape\n",
    "    masks = torch.sum(masks, 2)\n",
    "    masks = torch.min(masks, torch.ones_like(masks))\n",
    "    masks = torch.sum(masks, 1)\n",
    "    masks = masks - torch.ones_like(masks)\n",
    "    masks = masks.unsqueeze(1).expand(batch_size, embedding_dim).unsqueeze(1)\n",
    "    masks = torch.max(masks, torch.zeros_like(masks)) \n",
    "    last_visit = torch.gather(hidden_states, 1, masks)\n",
    "    last_visit = torch.flatten(last_visit, 1, 2)\n",
    "    return last_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveRNN(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.em = nn.Embedding(num_embeddings = num_codes, embedding_dim = 128)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size = 128, hidden_size = 128, batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(128, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, masks):\n",
    "\n",
    "        x = self.em(x)\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        x , _ = self.rnn(x)\n",
    "        x = get_last_visit(x, masks)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = x.view(-1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "naive_rnn = NaiveRNN(num_codes = len(diagnoses))\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_probs = []\n",
    "    \n",
    "    for step, batch in enumerate(val_loader):\n",
    "        x, masks, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            probs = model(x, masks)\n",
    "            val_labels.extend(labels.detach().tolist())\n",
    "            val_probs.extend(probs.detach().numpy().reshape(-1).tolist())\n",
    "            \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(val_labels, np.array(val_probs) > 0.5, average='binary')\n",
    "    roc_auc = roc_auc_score(val_labels, val_probs)\n",
    "    \n",
    "    print(f\"roc_auc:{roc_auc:3f}, precision:{precision:.3f},recall:{recall:3f},f1:{f1:3f}\")\n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            x, masks, labels = batch\n",
    "            \n",
    "            y_hat = model.forward(x, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(y_hat, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "            \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "#         print(train_loss)\n",
    "        eval_model(model, val_loader)\n",
    "\n",
    "    \n",
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
